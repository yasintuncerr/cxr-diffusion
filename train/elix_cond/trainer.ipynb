{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELIXRB 32x768 BASED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "from dotenv import load_dotenv \n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path(os.path.abspath('')).parent.parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "NIH_CXR14_DATASET_DIR = os.getenv(\"NIH_CXR14_DATASET_DIR\")\n",
    "print(NIH_CXR14_DATASET_DIR)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import Unet2DConditionalTrainerV2, TrainConfigV2, get_validation_samples_v2, create_validation_dataloader\n",
    "from diffusers import DDPMScheduler, UNet2DConditionModel\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "class EmbeddingLatentDataset(Dataset):\n",
    "    def __init__(self, latent_dir, embedding_dir, csv_path):\n",
    "        \"\"\"\n",
    "        Memory-efficient dataset that loads latent vectors and embeddings on demand,\n",
    "        filtered by a CSV file with image IDs.\n",
    "        \n",
    "        Args:\n",
    "            latent_dir (str): Directory containing latent vector H5 files\n",
    "            embedding_dir (str): Directory containing embedding H5 files\n",
    "            csv_path (str): Path to the CSV file containing image IDs to filter by\n",
    "        \"\"\"\n",
    "        # Store directories\n",
    "        self.latent_dir = latent_dir\n",
    "        self.embedding_dir = embedding_dir\n",
    "        \n",
    "        # Read CSV file with selected image IDs\n",
    "        self.selected_image_df = pd.read_csv(csv_path)\n",
    "        self.selected_image_ids = self.selected_image_df['Image Index'].tolist()\n",
    "        print(f\"Loaded {len(self.selected_image_ids)} image IDs from CSV\")\n",
    "        \n",
    "        # Get all file paths\n",
    "        self.latent_files = [os.path.join(latent_dir, f) for f in os.listdir(latent_dir) if f.endswith(\".h5\")]\n",
    "        self.embedding_files = [os.path.join(embedding_dir, f) for f in os.listdir(embedding_dir) if f.endswith(\".h5\")]\n",
    "        \n",
    "        # Build index of image IDs to file locations\n",
    "        self.latent_id_map = self._build_id_map(self.latent_files, \"latents\")\n",
    "        self.embedding_id_map = self._build_id_map(self.embedding_files, \"img_emb\")\n",
    "        \n",
    "        # Find common IDs between latents, embeddings, and the CSV\n",
    "        csv_set = set(self.selected_image_ids)\n",
    "        latent_set = set(self.latent_id_map.keys())\n",
    "        embedding_set = set(self.embedding_id_map.keys())\n",
    "        \n",
    "        # Filter to only include images in our CSV\n",
    "        self.common_ids = list(csv_set & latent_set & embedding_set)\n",
    "        \n",
    "        # Keep the order from the CSV\n",
    "        self.common_ids = [img_id for img_id in self.selected_image_ids if img_id in self.common_ids]\n",
    "        \n",
    "        print(f\"Total matching IDs after filtering: {len(self.common_ids)}\")\n",
    "        \n",
    "        # Statistics\n",
    "        print(f\"IDs in CSV but missing latents: {len(csv_set - latent_set)}\")\n",
    "        print(f\"IDs in CSV but missing embeddings: {len(csv_set - embedding_set)}\")\n",
    "    \n",
    "    def _build_id_map(self, files, dataset_name):\n",
    "        \"\"\"\n",
    "        Build a mapping from image IDs to file paths and indices within those files.\n",
    "        \n",
    "        Args:\n",
    "            files (list): List of H5 file paths\n",
    "            dataset_name (str): Name of dataset in H5 files (\"Latents\" or \"Embeddings\")\n",
    "            \n",
    "        Returns:\n",
    "            dict: Mapping from image ID to (file_path, index) tuple\n",
    "        \"\"\"\n",
    "        id_map = {}\n",
    "        \n",
    "        for file_path in files:\n",
    "            with h5py.File(file_path, \"r\") as hf:\n",
    "                try:\n",
    "                    image_indices = hf[\"Image index\"][:]\n",
    "                except:\n",
    "                    image_indices = hf[\"Image Index\"][:]\n",
    "                # Convert byte strings to regular strings if needed\n",
    "                if isinstance(image_indices[0], bytes):\n",
    "                    image_indices = [idx.decode(\"utf-8\") for idx in image_indices]\n",
    "                \n",
    "                # Map each ID to its file and position\n",
    "                for i, img_id in enumerate(image_indices):\n",
    "                    id_map[img_id] = (file_path, i)\n",
    "        \n",
    "        return id_map\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of matched image pairs\"\"\"\n",
    "        return len(self.common_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a latent and embedding pair by index.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index in the dataset\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (latent, cond_embed) pair as torch tensors\n",
    "        \"\"\"\n",
    "        # Get the image ID for this index\n",
    "        img_id = self.common_ids[idx]\n",
    "        \n",
    "        # Get file path and index for latent\n",
    "        latent_file, latent_idx = self.latent_id_map[img_id]\n",
    "        \n",
    "        # Get file path and index for embedding\n",
    "        embedding_file, embedding_idx = self.embedding_id_map[img_id]\n",
    "        \n",
    "        # Load latent vector\n",
    "        with h5py.File(latent_file, \"r\") as hf:\n",
    "            latent = hf[\"latents\"][latent_idx]\n",
    "        \n",
    "        # Load embedding\n",
    "        with h5py.File(embedding_file, \"r\") as hf:\n",
    "            cond_embed = hf[\"img_emb\"][embedding_idx]\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        latent = torch.tensor(latent, dtype=torch.float32)\n",
    "        cond_embed = torch.tensor(cond_embed, dtype=torch.float32)\n",
    "        \n",
    "        # Ensure conditioning embeddings have the right shape\n",
    "        # This is crucial for the UNet cross-attention to work properly\n",
    "        cond_embed = cond_embed.squeeze(0)\n",
    "           \n",
    "        return latent, cond_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elixrb_dir = os.path.join(NIH_CXR14_DATASET_DIR, \"elixr\", \"elixrb\", \"img_emb\")\n",
    "latent_dir = os.path.join(NIH_CXR14_DATASET_DIR, \"vae_latents2\")\n",
    "csv_path = \"/home/yasin/Lfstorage/Projects/cxr-diffusion/intermediate_data/filtered_findings_label_data.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EmbeddingLatentDataset(latent_dir, elixrb_dir, csv_path)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TrainConfigV2(\n",
    "    batch_size=4,\n",
    "    mixed_precision=True,\n",
    "    learning_rate=1e-5,\n",
    "    num_epochs=20,\n",
    "    gradient_accumulation_steps=64,  # Effective batch size of 128\n",
    "    scheduler_type=\"cosine\",\n",
    "    early_stopping_patience=5,\n",
    "    use_timestep_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet2DConditionModel(act_fn=\"silu\",\n",
    "                                    attention_head_dim=8,\n",
    "                                    center_input_sample=False,\n",
    "                                    downsample_padding=1,\n",
    "                                    flip_sin_to_cos=True,\n",
    "                                    freq_shift=0,\n",
    "                                    mid_block_scale_factor=1,\n",
    "                                    norm_eps=1e-05,\n",
    "                                    norm_num_groups=32,\n",
    "                                    sample_size=64, # generated samples are 512x512\n",
    "                                    in_channels=4, \n",
    "                                    out_channels=4, \n",
    "                                    layers_per_block=2, \n",
    "                                    block_out_channels=(320, 640, 1280, 1280), \n",
    "                                    down_block_types=(\n",
    "                                    \"CrossAttnDownBlock2D\",\n",
    "                                    \"CrossAttnDownBlock2D\",\n",
    "                                    \"CrossAttnDownBlock2D\",\n",
    "                                    \"DownBlock2D\"), \n",
    "                                    up_block_types=(\"UpBlock2D\",\n",
    "                                    \"CrossAttnUpBlock2D\",\n",
    "                                    \"CrossAttnUpBlock2D\",\n",
    "                                    \"CrossAttnUpBlock2D\"),\n",
    "                                    cross_attention_dim=768\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=config.learning_rate)\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.batch_size, num_workers=config.num_workers, shuffle=True, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_samples = get_validation_samples_v2(dataset, num_samples=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validation_samples[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard\n",
    "trainer = Unet2DConditionalTrainerV2(\n",
    "    unet=unet,\n",
    "    train_config=config,\n",
    "    noise_scheduler=noise_scheduler,\n",
    "    optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(dataloader = train_dataloader,  \n",
    "              validation_samples = validation_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
