{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\")) \n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "IMAGE_SIZE = 224\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA ANALYZE\n",
    "This section aims to analyze the NIH-CXR14 dataset through these operations:\n",
    "\n",
    "* **NIHDataset**: Loads tabular data and image paths from NIH-CXR14 dataset \n",
    "\n",
    "* **Field Filtering**: Selects essential columns `[\"Image Index\", \"Finding Labels\", \"Image Path\"]`\n",
    "\n",
    "* **Balance Adjustment**: Handles class imbalance using the `Finding Labels` field\n",
    "\n",
    "* **Label Exclusion**: Sets aside specific labels for validation using unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets import NIHDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nih_dataset = NIHDataset(root_dir = os.getenv(\"NIH_CXR14_DATASET_DIR\"), img_size = 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112120"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nih_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Image Index', 'Finding Labels', 'Follow-up #', 'Patient ID', 'Patient Age', 'Patient Gender', 'View Position', 'OriginalImage[Width', 'Height]', 'OriginalImagePixelSpacing[x', 'y]', 'Unnamed: 11', 'Image Path']\n"
     ]
    }
   ],
   "source": [
    "fields = nih_dataset.get_fields()\n",
    "print(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_filtered_dataset = nih_dataset.select_columns([\"Image Index\", \"Finding Labels\", \"Image Path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Image Index', 'Finding Labels', 'Image Path']\n"
     ]
    }
   ],
   "source": [
    "fields = field_filtered_dataset.get_fields()\n",
    "print(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Effusion': 13317, 'Emphysema': 2516, 'Infiltration': 19894, 'Pleural_Thickening': 3385, 'No Finding': 60361, 'Fibrosis': 1686, 'Mass': 5782, 'Atelectasis': 11559, 'Pneumothorax': 5302, 'Consolidation': 4667, 'Nodule': 6331, 'Cardiomegaly': 2776, 'Edema': 2303, 'Pneumonia': 1431, 'Hernia': 227}\n"
     ]
    }
   ],
   "source": [
    "label_counts = nih_dataset.get_label_counts()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'No Finding': 20000, 'Effusion': 13317, 'Emphysema': 2516, 'Infiltration': 19894, 'Pleural_Thickening': 3385, 'Fibrosis': 1686, 'Mass': 5782, 'Atelectasis': 11559, 'Pneumothorax': 5302, 'Consolidation': 4667, 'Nodule': 6331, 'Cardiomegaly': 2776, 'Edema': 2303, 'Pneumonia': 1431, 'Hernia': 227}\n"
     ]
    }
   ],
   "source": [
    "# apply limit for \"No Finding\" label\n",
    "dataset = field_filtered_dataset.limit_samples(label=\"No Finding\", max_samples=20000)\n",
    "label_counts = dataset.get_label_counts()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'No Finding': 20000, 'Effusion': 10027, 'Emphysema': 2088, 'Infiltration': 16602, 'Pleural_Thickening': 2882, 'Fibrosis': 1459, 'Mass': 5020, 'Pneumothorax': 4525, 'Consolidation': 3440, 'Nodule': 5734, 'Cardiomegaly': 2399, 'Edema': 2080, 'Pneumonia': 1166}\n"
     ]
    }
   ],
   "source": [
    "# drop some diseases labels to try the model can predict not trained labels\n",
    "exclude_labels = [\"Hernia\", 'Atelectasis']\n",
    "\n",
    "for label in exclude_labels:\n",
    "    dataset = dataset.filter_by_label(label, exclude=True)\n",
    "\n",
    "label_counts = dataset.get_label_counts()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Image-Text Pair Dataset\n",
    "**these will be used for feeding `vae` and `clip-text`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "\n",
    "class IMGTEXTDataset(Dataset):\n",
    "    def __init__(self, dataset, transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: The NIH dataset instance\n",
    "            transforms: torchvision transforms to be applied to images\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        \n",
    "        # Get image path and labels\n",
    "        image_path = sample[\"Image Path\"]\n",
    "        labels = sample[\"Finding Labels\"]\n",
    "        \n",
    "        # Open image and keep it as PIL Image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Apply transforms if any\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)  # Apply transforms directly to PIL Image\n",
    "            \n",
    "        # Join labels into a sentence\n",
    "        sentence = \" \".join(labels)\n",
    "        \n",
    "        return image, sentence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Latent-TextEmbed Pair Dataset\n",
    "\n",
    "**These will be used when training diffision model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasin/Projects/elixr-diffusion/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from src.pipelines import VaeProcessor, CLIPTextProcessor\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda:2\"\n",
    "\n",
    "\n",
    "vae_processor = VaeProcessor(device=device) # optional vae model can be passed as argument\n",
    "clip_processor = CLIPTextProcessor(device=device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 400 samples\n",
      "Processed 800 samples\n",
      "Processed 1200 samples\n",
      "Processed 1600 samples\n",
      "Processed 2000 samples\n",
      "Processed 2400 samples\n",
      "Processed 2800 samples\n",
      "Processed 3200 samples\n",
      "Processed 3600 samples\n",
      "Processed 4000 samples\n",
      "Processed 4400 samples\n",
      "Processed 4800 samples\n",
      "Processed 5200 samples\n",
      "Processed 5600 samples\n",
      "Processed 6000 samples\n",
      "Processed 6400 samples\n",
      "Processed 6800 samples\n",
      "Processed 7200 samples\n",
      "Processed 7600 samples\n",
      "Processed 8000 samples\n",
      "Processed 8400 samples\n",
      "Processed 8800 samples\n",
      "Processed 9200 samples\n",
      "Processed 9600 samples\n",
      "Processed 10000 samples\n",
      "Processed 10400 samples\n",
      "Processed 10800 samples\n",
      "Processed 11200 samples\n",
      "Processed 11600 samples\n",
      "Processed 12000 samples\n",
      "Processed 12400 samples\n",
      "Processed 12800 samples\n",
      "Processed 13200 samples\n",
      "Processed 13600 samples\n",
      "Processed 14000 samples\n",
      "Processed 14400 samples\n",
      "Processed 14800 samples\n",
      "Processed 15200 samples\n",
      "Processed 15600 samples\n",
      "Processed 16000 samples\n",
      "Processed 16400 samples\n",
      "Processed 16800 samples\n",
      "Processed 17200 samples\n",
      "Processed 17600 samples\n",
      "Processed 18000 samples\n",
      "Processed 18400 samples\n",
      "Processed 18800 samples\n",
      "Processed 19200 samples\n",
      "Processed 19600 samples\n",
      "Processed 20000 samples\n",
      "Processed 20400 samples\n",
      "Processed 20800 samples\n",
      "Processed 21200 samples\n",
      "Processed 21600 samples\n",
      "Processed 22000 samples\n",
      "Processed 22400 samples\n",
      "Processed 22800 samples\n",
      "Processed 23200 samples\n",
      "Processed 23600 samples\n",
      "Processed 24000 samples\n",
      "Processed 24400 samples\n",
      "Processed 24800 samples\n",
      "Processed 25200 samples\n",
      "Processed 25600 samples\n",
      "Processed 26000 samples\n",
      "Processed 26400 samples\n",
      "Processed 26800 samples\n",
      "Processed 27200 samples\n",
      "Processed 27600 samples\n",
      "Processed 28000 samples\n",
      "Processed 28400 samples\n",
      "Processed 28800 samples\n",
      "Processed 29200 samples\n",
      "Processed 29600 samples\n",
      "Processed 30000 samples\n",
      "Processed 30400 samples\n",
      "Processed 30800 samples\n",
      "Processed 31200 samples\n",
      "Processed 31600 samples\n",
      "Processed 32000 samples\n",
      "Processed 32400 samples\n",
      "Processed 32800 samples\n",
      "Processed 33200 samples\n",
      "Processed 33600 samples\n",
      "Processed 34000 samples\n",
      "Processed 34400 samples\n",
      "Processed 34800 samples\n",
      "Processed 35200 samples\n",
      "Processed 35600 samples\n",
      "Processed 36000 samples\n",
      "Processed 36400 samples\n",
      "Processed 36800 samples\n",
      "Processed 37200 samples\n",
      "Processed 37600 samples\n",
      "Processed 38000 samples\n",
      "Processed 38400 samples\n",
      "Processed 38800 samples\n",
      "Processed 39200 samples\n",
      "Processed 39600 samples\n",
      "Processed 40000 samples\n",
      "Processed 40400 samples\n",
      "Processed 40800 samples\n",
      "Processed 41200 samples\n",
      "Processed 41600 samples\n",
      "Processed 42000 samples\n",
      "Processed 42400 samples\n",
      "Processed 42800 samples\n",
      "Processed 43200 samples\n",
      "Processed 43600 samples\n",
      "Processed 44000 samples\n",
      "Processed 44400 samples\n",
      "Processed 44800 samples\n",
      "Processed 45200 samples\n",
      "Processed 45600 samples\n",
      "Processed 46000 samples\n",
      "Processed 46400 samples\n",
      "Processed 46800 samples\n",
      "Processed 47200 samples\n",
      "Processed 47600 samples\n",
      "Processed 48000 samples\n",
      "Processed 48400 samples\n",
      "Processed 48800 samples\n",
      "Processed 49200 samples\n",
      "Processed 49600 samples\n",
      "Processed 50000 samples\n",
      "Processed 50400 samples\n",
      "Processed 50800 samples\n",
      "Processed 51200 samples\n",
      "Processed 51600 samples\n",
      "Processed 52000 samples\n",
      "Processed 52400 samples\n",
      "Processed 52800 samples\n",
      "Processed 53200 samples\n",
      "Processed 53600 samples\n",
      "Processed 54000 samples\n",
      "Processed 54400 samples\n",
      "Processed 54800 samples\n",
      "Processed 55200 samples\n",
      "Processed 55600 samples\n",
      "Processed 56000 samples\n",
      "Processed 56400 samples\n",
      "Processed 56800 samples\n",
      "Processed 57200 samples\n",
      "Processed 57600 samples\n",
      "Processed 58000 samples\n",
      "Processed 58400 samples\n",
      "Processed 58800 samples\n",
      "Processed 59200 samples\n",
      "Processed 59600 samples\n",
      "Processed 60000 samples\n",
      "Total samples processed: 60013\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "# Use batch processing for efficiency\n",
    "dataloader = DataLoader(IMGTEXTDataset(dataset, image_transform), batch_size=16, shuffle=False)\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "save_dir = Path(\"data\")\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Initialize storage dictionary\n",
    "embeddings = {\n",
    "    'latents': {},\n",
    "    'texts': {}\n",
    "}\n",
    "\n",
    "current_idx = 0\n",
    "for images, texts in dataloader:\n",
    "    # Process batch\n",
    "\n",
    "    images = images.to(vae_processor.device)\n",
    "\n",
    "    # Get text embeddings\n",
    "    text_embeddings = clip_processor.encode_text(texts)\n",
    "\n",
    "\n",
    "    # Get image latents\n",
    "    image_latents = vae_processor.prepare_latent(images)\n",
    "\n",
    "    \n",
    "    # Store individual samples from the batch\n",
    "    for i in range(len(images)):\n",
    "        embeddings['latents'][current_idx] = image_latents[i].detach().cpu().numpy()\n",
    "        embeddings['texts'][current_idx] = text_embeddings[i].detach().cpu().numpy()\n",
    "        current_idx += 1\n",
    "    \n",
    "    if current_idx % 100 == 0:\n",
    "        print(f\"Processed {current_idx} samples\")\n",
    "        # Periodically save to avoid memory issues\n",
    "        with open(save_dir / \"embeddings.pkl\", \"wb\") as f:\n",
    "            pickle.dump(embeddings, f)\n",
    "\n",
    "# Final save\n",
    "with open(save_dir / \"embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "\n",
    "print(f\"Total samples processed: {current_idx}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
