{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get absolute path to project root\n",
    "project_root = Path(os.path.abspath('')).parent.parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "nih_dataset_root_dir = os.getenv(\"NIH_CXR14_DATASET_DIR\")\n",
    "\n",
    "main_output_dir = \"../data\"\n",
    "os.makedirs(main_output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets import NIHFindingLabels\n",
    "\n",
    "\n",
    "nih_finding_labels = NIHFindingLabels.load_from_processed(main_output_dir)\n",
    "\n",
    "\n",
    "#print sample of the labels\n",
    "\n",
    "sample = nih_finding_labels[0]\n",
    "\n",
    "print(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, label_dataset):\n",
    "        self.label_dataset = label_dataset\n",
    "        # Convert dict_keys to list for indexing\n",
    "        self.str_labels = list(label_dataset.label_counts.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        binary_labels, image_id, _ = self.label_dataset[idx]\n",
    "\n",
    "        # Build sentence from positive labels\n",
    "        sentences = []\n",
    "        for i, valid in enumerate(binary_labels):\n",
    "            if valid:\n",
    "                sentences.append(self.str_labels[i])\n",
    "\n",
    "        return image_id, \", \".join(sentences)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "batch_size = 64\n",
    "num_workers = 24\n",
    "shuffle = False\n",
    "pin_memory = True\n",
    "\n",
    "dataset = CustomDataset(nih_finding_labels)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipelines import CLIPTextProcessor\n",
    "\n",
    "\n",
    "clip_text_processor = CLIPTextProcessor(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "def update_pickle(pickle_file, data):\n",
    "    \n",
    "    try:\n",
    "        # If file exists, load and update\n",
    "        if os.path.exists(pickle_file):\n",
    "            with open(pickle_file, 'rb') as f:\n",
    "                old_data = pickle.load(f)\n",
    "            old_data.update(data)\n",
    "        else:\n",
    "            # If file doesn't exist, use new data directly\n",
    "            old_data = data\n",
    "        \n",
    "        # Save updated data\n",
    "        with open(pickle_file, 'wb') as f:\n",
    "            pickle.dump(old_data, f)\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error updating pickle file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import torch\n",
    "\n",
    "output_file_path = os.path.join(main_output_dir, \"clip_text_embeds.pkl\")\n",
    "text_embeds = {}\n",
    "\n",
    "try:\n",
    "    for image_ids, sentences in tqdm(dataloader):\n",
    "        # Generate embeddings\n",
    "        embeds = clip_text_processor.encode_text(sentences)\n",
    "        embeds = embeds.detach().cpu().numpy()\n",
    "        \n",
    "        # Store embeddings for each image\n",
    "        for i, image_id in enumerate(image_ids):\n",
    "            text_embeds[image_id] = embeds[i]\n",
    "            \n",
    "        # Save periodically to avoid data loss\n",
    "        if len(text_embeds) % 1000 == 0:\n",
    "            print(f\"\\nSaving {len(text_embeds)} embeddings...\")\n",
    "            update_pickle(output_file_path, text_embeds)\n",
    "            text_embeds = {}  # Clear memory\n",
    "\n",
    "    # Save any remaining embeddings\n",
    "    if text_embeds:\n",
    "        print(f\"\\nSaving final {len(text_embeds)} embeddings...\")\n",
    "        update_pickle(output_file_path, text_embeds)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in generating text embeddings: {str(e)}\")\n",
    "    \n",
    "    # Emergency save of any processed embeddings\n",
    "    if text_embeds:\n",
    "        emergency_path = os.path.join(main_output_dir, \"clip_text_embeds_emergency.pkl\")\n",
    "        print(f\"Attempting emergency save to {emergency_path}\")\n",
    "        try:\n",
    "            update_pickle(emergency_path, text_embeds)\n",
    "        except Exception as save_error:\n",
    "            print(f\"Emergency save failed: {str(save_error)}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
