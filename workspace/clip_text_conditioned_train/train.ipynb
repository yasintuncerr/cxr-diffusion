{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from diffusers import DDPMScheduler, UNet2DConditionModel\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "# Get absolute path to project root\n",
    "project_root = Path(os.path.abspath('')).parent.parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "nih_dataset_root_dir = os.getenv(\"NIH_CXR14_DATASET_DIR\")\n",
    "\n",
    "main_data_dir = \"../data\"\n",
    "\n",
    "\n",
    "\n",
    "from src.pipelines import VaeProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_config = {\n",
    "    # Architecture parameters\n",
    "    \"sample_size\": 28,  # for 224x224 images (224 = 28 * 8)\n",
    "    \"in_channels\": 4,\n",
    "    \"out_channels\": 4,\n",
    "    \"layers_per_block\": 2,\n",
    "    \"block_out_channels\": (320, 640, 1280, 1280),\n",
    "    \"down_block_types\": (\n",
    "        \"CrossAttnDownBlock2D\",\n",
    "        \"CrossAttnDownBlock2D\",\n",
    "        \"CrossAttnDownBlock2D\",\n",
    "        \"DownBlock2D\"\n",
    "    ),\n",
    "    \"up_block_types\": (\n",
    "        \"UpBlock2D\",\n",
    "        \"CrossAttnUpBlock2D\",\n",
    "        \"CrossAttnUpBlock2D\",\n",
    "        \"CrossAttnUpBlock2D\"\n",
    "    ),\n",
    "    \n",
    "    # Attention parameters\n",
    "    \"attention_head_dim\": 8,\n",
    "    \"cross_attention_dim\": 768,\n",
    "    \n",
    "    # Normalization and activation\n",
    "    \"norm_num_groups\": 32,\n",
    "    \"norm_eps\": 1e-05,\n",
    "    \"act_fn\": \"silu\",\n",
    "    \n",
    "    # Additional configuration\n",
    "    \"center_input_sample\": False,\n",
    "    \"downsample_padding\": 1,\n",
    "    \"flip_sin_to_cos\": True,\n",
    "    \"freq_shift\": 0,\n",
    "    \"mid_block_scale_factor\": 1\n",
    "}\n",
    "\n",
    "\n",
    "# To create the model:\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class train_config:\n",
    "    device = \"cuda:2\"\n",
    "    num_workers = 32\n",
    "    batch_size = 32\n",
    "    mixed_precision = \"fp16\"    \n",
    "    output_dir = \"output\"\n",
    "    save_model_epochs = 1\n",
    "    num_epochs = 20\n",
    "    num_train_timesteps = 1000\n",
    "    learning_rate = 1e-5\n",
    "    lr_warmup_steps:int = 500\n",
    "    unet_config = unet_config\n",
    "    gradient_accumulation_steps: int = 1\n",
    "\n",
    "    \n",
    "\n",
    "torch.cuda.set_device(int(train_config.device.split(\":\")[-1]))\n",
    "config = train_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = pickle.load(open(f\"{main_data_dir}/latents.pkl\", \"rb\"))\n",
    "\n",
    "text_embeds = pickle.load(open(f\"{main_data_dir}/clip_text_embeds.pkl\", \"rb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, latents, text_embeds, latent_transform=None, text_embed_transform=None):\n",
    "        self.latents = latents\n",
    "        self.text_embeds = text_embeds\n",
    "        self._keys = list(latents.keys())\n",
    "        self.text_transform = text_embed_transform\n",
    "        self.latent_transform = latent_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._keys)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        key = self._keys[idx]\n",
    "\n",
    "        # Convert text_embed to tensor if it's numpy array\n",
    "        text_embed = self.text_embeds[key]\n",
    "        if isinstance(text_embed, np.ndarray):\n",
    "            text_embed = torch.from_numpy(text_embed).float()\n",
    "\n",
    "        # Pad or trim text embeddings\n",
    "        if text_embed.shape[0] < 77:\n",
    "            padding = torch.zeros(77 - text_embed.shape[0], 768)\n",
    "            text_embed = torch.cat((text_embed, padding), 0)\n",
    "        elif text_embed.shape[0] > 77:\n",
    "            text_embed = text_embed[:77]\n",
    "\n",
    "        if self.text_transform:\n",
    "            text_embed = self.text_transform(text_embed)\n",
    "        \n",
    "        # Convert latent to tensor if it's numpy array\n",
    "        latent = self.latents[key]\n",
    "        if isinstance(latent, np.ndarray):\n",
    "            latent = torch.from_numpy(latent).float()\n",
    "\n",
    "        if self.latent_transform:\n",
    "            latent = self.latent_transform(latent)\n",
    "\n",
    "        return latent, text_embed\n",
    "\n",
    "def save_images_grid(images, output_path, grid_size=None, padding=10):\n",
    "    \n",
    "    if not images:\n",
    "        raise ValueError(\"Image list is empty\")\n",
    "    \n",
    "    if grid_size is None:\n",
    "        n_images = len(images)\n",
    "        n_cols = math.ceil(math.sqrt(n_images))\n",
    "        n_rows = math.ceil(n_images / n_cols)\n",
    "    else:\n",
    "        n_rows, n_cols = grid_size\n",
    "        \n",
    "    max_width = max(img.width for img in images)\n",
    "    max_height = max(img.height for img in images)\n",
    "    \n",
    "    grid_width = (max_width + padding) * n_cols - padding\n",
    "    grid_height = (max_height + padding) * n_rows - padding\n",
    "    grid_image = Image.new('RGB', (grid_width, grid_height), color='white')\n",
    "    \n",
    "    for idx, img in enumerate(images):\n",
    "        row = idx // n_cols\n",
    "        col = idx % n_cols\n",
    "        \n",
    "        x = col * (max_width + padding)\n",
    "        y = row * (max_height + padding)\n",
    "        \n",
    "        x_offset = (max_width - img.width) // 2\n",
    "        y_offset = (max_height - img.height) // 2\n",
    "        \n",
    "        grid_image.paste(img, (x + x_offset, y + y_offset))\n",
    "    \n",
    "    grid_image.save(output_path, quality=95)\n",
    "    return grid_image\n",
    "def generate_sample(model, samples, device):\n",
    "    scheduler = DDPMScheduler()\n",
    "    scheduler.set_timesteps(25)\n",
    "    \n",
    "    model.eval()\n",
    "    vae_processor = VaeProcessor(device)\n",
    "    images = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sample in samples:\n",
    "            latent, text_embed = sample\n",
    "            \n",
    "            # Ensure correct shape for latents [batch_size, channels, height, width]\n",
    "            if len(latent.shape) == 3:\n",
    "                latent = latent.unsqueeze(0)  # Add batch dimension if missing\n",
    "            \n",
    "            # Move to device\n",
    "            latent = latent.to(device)\n",
    "            text_embed = text_embed.to(device)\n",
    "            \n",
    "            # Ensure text embeddings have correct shape [batch_size, sequence_length, hidden_size]\n",
    "            if len(text_embed.shape) == 2:\n",
    "                text_embed = text_embed.unsqueeze(0)\n",
    "            \n",
    "            \n",
    "            # Initialize noise\n",
    "            latent = latent * scheduler.init_noise_sigma\n",
    "            \n",
    "            # Denoising loop\n",
    "            for t in scheduler.timesteps:\n",
    "                latent_model_input = scheduler.scale_model_input(latent, t)\n",
    "                \n",
    "                noise_pred = model(\n",
    "                    latent_model_input,\n",
    "                    t,\n",
    "                    encoder_hidden_states=text_embed,\n",
    "                    return_dict=False\n",
    "                )[0]\n",
    "                \n",
    "                latent = scheduler.step(noise_pred, t, latent).prev_sample\n",
    "            \n",
    "            # Decode and append the generated image\n",
    "            images.append(vae_processor.decode_latent(latent))\n",
    "    \n",
    "    del vae_processor\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(latents, text_embeds, latent_transform=None, text_embed_transform=None)\n",
    "\n",
    "\n",
    "\n",
    "fixed_validate_samples = []\n",
    "\n",
    "random_idx = torch.randint(0, len(dataset), (10,))\n",
    "\n",
    "for idx in random_idx:\n",
    "    fixed_validate_samples.append(dataset[idx])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, \n",
    "                        batch_size=config.batch_size, \n",
    "                        shuffle=True, \n",
    "                        num_workers=config.num_workers,\n",
    "                        pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fixed_validate_samples[0][0].shape, fixed_validate_samples[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet2DConditionModel(**unet_config)\n",
    "\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=config.num_train_timesteps)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(optimizer, \n",
    "                                               num_warmup_steps=config.lr_warmup_steps, \n",
    "                                               num_training_steps=len(dataset) * config.num_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "import tensorboard as tb\n",
    "\n",
    "# Initialize accelerator and tensorboard logging\n",
    "accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        log_with=\"tensorboard\",\n",
    "        project_dir=os.path.join(config.output_dir, \"logs\"),\n",
    "    )\n",
    "if accelerator.is_main_process:\n",
    "    if config.output_dir is not None:\n",
    "        os.makedirs(config.output_dir, exist_ok=True)\n",
    "    accelerator.init_trackers(\"train_example\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, lr_scheduler, dataloader,  = accelerator.prepare(\n",
    "    model, optimizer, lr_scheduler, dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_checkpoint(model, accelerator, output_dir, epoch):\n",
    "    # Unwrap the model from accelerator to get the original model\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    \n",
    "    # Save the model state\n",
    "    if accelerator.is_main_process:\n",
    "        # Create pipeline folder\n",
    "        os.makedirs(os.path.join(output_dir, f\"checkpoint-{epoch}\"), exist_ok=True)\n",
    "        \n",
    "        # Save the model in diffusers format\n",
    "        unwrapped_model.save_pretrained(os.path.join(output_dir, f\"checkpoint-{epoch}/unet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, model, noise_scheduler, dataloader, optimizer, lr_scheduler, accelerator):\n",
    "    \"\"\"\n",
    "    Training loop for diffusion model.\n",
    "    \"\"\"\n",
    "    global_step = 0\n",
    "    \n",
    "    # Progress bar for epochs\n",
    "    progress_bar = tqdm(range(config.num_epochs), disable=not accelerator.is_local_main_process)\n",
    "    progress_bar.set_description(\"Epochs\")\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        # Progress bar for steps\n",
    "        step_progress_bar = tqdm(total=len(dataloader), disable=not accelerator.is_local_main_process)\n",
    "        step_progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "        \n",
    "        # Track epoch metrics\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for step, (latents, text_embeds) in enumerate(dataloader):\n",
    "            # Prepare inputs\n",
    "            latents = latents.to(accelerator.device)\n",
    "            text_embeds = text_embeds.to(accelerator.device)\n",
    "            \n",
    "            # Sample noise and timesteps\n",
    "            noise = torch.randn_like(latents)\n",
    "            timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (latents.shape[0],), device=latents.device).long()\n",
    "            \n",
    "            # Add noise to latents\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "            \n",
    "            # Training step\n",
    "            with accelerator.accumulate(model):\n",
    "                # Predict noise\n",
    "                noise_pred = model(\n",
    "                    noisy_latents,\n",
    "                    timesteps,\n",
    "                    encoder_hidden_states=text_embeds,\n",
    "                    return_dict=False\n",
    "                )[0]\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                epoch_loss += loss.detach().item()\n",
    "                \n",
    "                # Backpropagation\n",
    "                accelerator.backward(loss)\n",
    "                \n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            # Logging\n",
    "            if accelerator.is_main_process:\n",
    "                logs = {\n",
    "                    \"loss\": loss.detach().item(),\n",
    "                    \"avg_loss\": epoch_loss / (step + 1),\n",
    "                    \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "                    \"epoch\": epoch,\n",
    "                    \"step\": global_step,\n",
    "                }\n",
    "                step_progress_bar.set_postfix(**logs)\n",
    "                accelerator.log(logs, step=global_step)\n",
    "            \n",
    "            global_step += 1\n",
    "            step_progress_bar.update(1)\n",
    "            \n",
    "        \n",
    "        # Save checkpoint and generate samples\n",
    "        if (epoch + 1) % config.save_model_epochs == 0:\n",
    "            # Create output directory\n",
    "            output_dir = os.path.join(config.output_dir, \".cache\")\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            # Save model\n",
    "            save_model_checkpoint(model, accelerator, output_dir, epoch)\n",
    "            \n",
    "            # Generate and save samples\n",
    "            with torch.no_grad():\n",
    "                images = generate_sample(model, fixed_validate_samples, accelerator.device)\n",
    "                save_images_grid(images, os.path.join(output_dir, f\"sample_{epoch}.png\"))\n",
    "        \n",
    "        step_progress_bar.close()\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    save_model_checkpoint(model, accelerator, config.output_dir, \"final\")\n",
    "    images = generate_sample(model, fixed_validate_samples, \"cuda:2\")\n",
    "    save_images_grid(images, os.path.join(config.output_dir, \"final_sample.png\"))\n",
    "\n",
    "\n",
    "    progress_bar.close()\n",
    "\n",
    "    return global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(config, model, noise_scheduler, dataloader, optimizer, lr_scheduler, accelerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
